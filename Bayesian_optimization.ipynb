{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10951
    },
    "colab_type": "code",
    "id": "hvx74cPoAFdF",
    "outputId": "d869d2cd-b7ff-42b6-ba5b-d26b5b6d56ae"
   },
   "outputs": [],
   "source": [
    "# When running on colab, run below commands\n",
    "!mkdir dataset\n",
    "%cd dataset\n",
    "!wget https://cv.snu.ac.kr/research/VDSR/train_data.zip\n",
    "!wget https://cv.snu.ac.kr/research/VDSR/test_data.zip\n",
    "!unzip train_data.zip\n",
    "!unzip test_data.zip\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "yIFszr6AAI46",
    "outputId": "b927cade-455e-4a05-c136-31a3894e1b3b"
   },
   "outputs": [],
   "source": [
    "! pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "wm4IzsJsAFjy",
    "outputId": "0d703a88-38ec-4124-e2ad-60dcc62b522f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshitsrivastava/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing the required packages\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "from keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Add, PReLU, Conv2DTranspose, Concatenate, MaxPooling2D, UpSampling2D, Dropout\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import Callback, LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras.optimizers import *\n",
    "from keras.utils import Sequence\n",
    "# Using Keras Model in Scikit Learn\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# include below until https://github.com/scikit-optimize/scikit-optimize/issues/718 is resolved\n",
    "class BayesSearchCV(BayesSearchCV):\n",
    "    def _run_search(self, x): raise BaseException('Use newer skopt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mkWdB-4kJYwG"
   },
   "outputs": [],
   "source": [
    "def get_noise_model(noise_type=\"gaussian,0,50\"):\n",
    "    tokens = noise_type.split(sep=\",\")\n",
    "\n",
    "    if tokens[0] == \"gaussian\":\n",
    "        min_stddev = int(tokens[1])\n",
    "        max_stddev = int(tokens[2])\n",
    "\n",
    "        def gaussian_noise(img):\n",
    "            noise_img = img.astype(np.float)\n",
    "            stddev = np.random.uniform(min_stddev, max_stddev)\n",
    "            noise = np.random.randn(*img.shape) * stddev\n",
    "            noise_img += noise\n",
    "            noise_img = np.clip(noise_img, 0, 255).astype(np.uint8)\n",
    "            return noise_img\n",
    "\n",
    "        return gaussian_noise\n",
    "\n",
    "    elif tokens[0] == \"clean\":\n",
    "        return lambda img: img\n",
    "\n",
    "    elif tokens[0] == \"text\":\n",
    "\n",
    "        min_occupancy = int(tokens[1])\n",
    "        max_occupancy = int(tokens[2])\n",
    "\n",
    "        def add_text(img):\n",
    "\n",
    "            img = img.copy()\n",
    "            h, w, _ = img.shape\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            img_for_cnt = np.zeros((h, w), np.uint8)\n",
    "            occupancy = np.random.uniform(min_occupancy, max_occupancy)\n",
    "\n",
    "            while True:\n",
    "                n = random.randint(5, 10)\n",
    "                random_str = ''.join([random.choice(string.ascii_letters + string.digits) for i in range(n)])\n",
    "                font_scale = np.random.uniform(0.5, 1)\n",
    "                thickness = random.randint(1, 3)\n",
    "                (fw, fh), baseline = cv2.getTextSize(random_str, font, font_scale, thickness)\n",
    "                x = random.randint(0, max(0, w - 1 - fw))\n",
    "                y = random.randint(fh, h - 1 - baseline)\n",
    "                color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "                cv2.putText(img, random_str, (x, y), font, font_scale, color, thickness)\n",
    "                cv2.putText(img_for_cnt, random_str, (x, y), font, font_scale, 255, thickness)\n",
    "\n",
    "                if (img_for_cnt > 0).sum() > h * w * occupancy / 100:\n",
    "                    break\n",
    "            return img\n",
    "\n",
    "        return add_text\n",
    "\n",
    "\n",
    "    elif tokens[0] == \"impulse\":\n",
    "\n",
    "        min_occupancy = int(tokens[1])\n",
    "        max_occupancy = int(tokens[2])\n",
    "\n",
    "        def add_impulse_noise(img):\n",
    "            occupancy = np.random.uniform(min_occupancy, max_occupancy)\n",
    "            mask = np.random.binomial(size=img.shape, n=1, p=occupancy / 100)\n",
    "            noise = np.random.randint(256, size=img.shape)\n",
    "            img = img * (1 - mask) + noise * mask\n",
    "            return img.astype(np.uint8)\n",
    "        return add_impulse_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovYc7MH-JYwJ"
   },
   "outputs": [],
   "source": [
    "class NoisyImageGenerator(Sequence):\n",
    "  \n",
    "  def __init__(self, image_dir, source_noise_model, target_noise_model, batch_size=10000, image_size=128):\n",
    "\n",
    "    self.image_paths = list(Path(image_dir).glob(\"*.jpg\"))\n",
    "    self.source_noise_model = source_noise_model\n",
    "    self.target_noise_model = target_noise_model\n",
    "    self.image_num = len(self.image_paths)\n",
    "    self.batch_size = batch_size\n",
    "    self.image_size = image_size\n",
    "\n",
    "  def __len__(self):\n",
    "\n",
    "    return self.image_num // self.batch_size\n",
    "\n",
    "  def __getitem__(self):\n",
    "\n",
    "    batch_size = self.batch_size\n",
    "    image_size = self.image_size\n",
    "    x = np.zeros((batch_size, image_size, image_size, 3), dtype=np.uint8)\n",
    "    y = np.zeros((batch_size, image_size, image_size, 3), dtype=np.uint8)\n",
    "    sample_id = 0\n",
    "\n",
    "    while True:\n",
    "      \n",
    "      image_path = random.choice(self.image_paths)\n",
    "      image = cv2.imread(str(image_path))\n",
    "      h, w, _ = image.shape\n",
    "\n",
    "      if h >= image_size and w >= image_size:\n",
    "          h, w, _ = image.shape\n",
    "          i = np.random.randint(h - image_size + 1)\n",
    "          j = np.random.randint(w - image_size + 1)\n",
    "          clean_patch = image[i:i + image_size, j:j + image_size]\n",
    "          x[sample_id] = self.source_noise_model(clean_patch)\n",
    "          y[sample_id] = self.target_noise_model(clean_patch)\n",
    "\n",
    "          sample_id += 1\n",
    "\n",
    "          if sample_id == batch_size:\n",
    "              return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-wGPxtoeJYwL"
   },
   "outputs": [],
   "source": [
    "class Generator(Sequence):\n",
    "    def __init__(self, image_dir, val_noise_model):\n",
    "\n",
    "        image_paths = list(Path(image_dir).glob(\"*.*\"))\n",
    "        self.image_num = len(image_paths)\n",
    "        self.data = []\n",
    "\n",
    "        for image_path in image_paths:\n",
    "            y = cv2.imread(str(image_path))\n",
    "            h, w, _ = y.shape\n",
    "            y = y[:(h // 16) * 16, :(w // 16) * 16]  # for stride (maximum 16)\n",
    "            x = val_noise_model(y)\n",
    "            self.data.append(x)\n",
    "            #print (x.shape)\n",
    "            #print(y.shape)\n",
    "            #self.data.append([np.expand_dims(x, axis=0), np.expand_dims(y, axis=0)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image_num\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJbjChdOJYwN"
   },
   "outputs": [],
   "source": [
    "source_noise_model = get_noise_model(\"text,0,50\")\n",
    "target_noise_model = get_noise_model( \"text,0,0\")\n",
    "val_noise_model = get_noise_model(\"text,0,50\")\n",
    "\n",
    "image_dir = \"dataset/291\"\n",
    "test_dir =  \"dataset/Set14\"\n",
    "\n",
    "# batch_size = 4\n",
    "# learning_rate = 0.003\n",
    "\n",
    "# noisy_generator = NoisyImageGenerator(image_dir, source_noise_model, target_noise_model, batch_size=batch_size,\n",
    "#                               image_size=64)\n",
    "# val_generator = ValGenerator(test_dir, val_noise_model)\n",
    "x, y = np.array(NoisyImageGenerator(image_dir, source_noise_model, target_noise_model).__getitem__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QpyJ9dcEJYwQ"
   },
   "outputs": [],
   "source": [
    "def tf_log10(x):\n",
    "    numerator = tf.log(x)\n",
    "    denominator = tf.log(tf.constant(10, dtype=numerator.dtype))\n",
    "    return numerator / denominator\n",
    "\n",
    "def PSNR(y_true, y_pred):\n",
    "    max_pixel = 255.0\n",
    "    y_pred = K.clip(y_pred, 0.0, 255.0)\n",
    "    return 10.0 * tf_log10((max_pixel ** 2) / (K.mean(K.square(y_pred - y_true))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xqw3GI3CJYwT"
   },
   "outputs": [],
   "source": [
    "def get_unet_model(activation, learning_rate, optimizer, input_channel_num=3, out_ch=3, start_ch=64, depth=4, \n",
    "                   inc_rate=2., dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n",
    "    def _conv_block(m, dim, acti, bn, res, do=0):\n",
    "        n = Conv2D(dim, 3, activation=acti, padding='same')(m)\n",
    "        n = BatchNormalization()(n) if bn else n\n",
    "        n = Dropout(do)(n) if do else n\n",
    "        n = Conv2D(dim, 3, activation=acti, padding='same')(n)\n",
    "        n = BatchNormalization()(n) if bn else n\n",
    "\n",
    "        return Concatenate()([m, n]) if res else n\n",
    "\n",
    "    def _level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n",
    "        if depth > 0:\n",
    "            n = _conv_block(m, dim, acti, bn, res)\n",
    "            m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
    "            m = _level_block(m, int(inc * dim), depth - 1, inc, acti, do, bn, mp, up, res)\n",
    "            if up:\n",
    "                m = UpSampling2D()(m)\n",
    "                m = Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
    "            else:\n",
    "                m = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n",
    "            n = Concatenate()([n, m])\n",
    "            m = _conv_block(n, dim, acti, bn, res)\n",
    "        else:\n",
    "            m = _conv_block(m, dim, acti, bn, res, do)\n",
    "\n",
    "        return m\n",
    "\n",
    "    i = Input(shape=(None, None, input_channel_num))\n",
    "    o = _level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
    "    o = Conv2D(out_ch, 1)(o)\n",
    "    model = Model(inputs=i, outputs=o)\n",
    "    \n",
    "    model.compile(optimizer=optimizer(lr=learning_rate), loss=\"mae\", metrics=[PSNR])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2448
    },
    "colab_type": "code",
    "id": "BgZw94Aw_-jI",
    "outputId": "af9160ac-d85a-45ec-aaf9-e20c096f8aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "[CV] batch_size=4, learning_rate=0.005384601561294807, optimizer=<class 'keras.optimizers.Adadelta'> \n",
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 378s 76ms/step - loss: 33.0064 - PSNR: 15.2019\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 369s 74ms/step - loss: 24.1226 - PSNR: 16.6232\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 366s 73ms/step - loss: 22.6009 - PSNR: 16.9106\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 368s 74ms/step - loss: 21.4859 - PSNR: 17.1646\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 364s 73ms/step - loss: 20.5184 - PSNR: 17.3970\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 367s 73ms/step - loss: 19.7330 - PSNR: 17.5958\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 371s 74ms/step - loss: 19.0987 - PSNR: 17.7661\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 374s 75ms/step - loss: 18.5506 - PSNR: 17.9092\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 379s 76ms/step - loss: 18.0710 - PSNR: 18.0267\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 373s 75ms/step - loss: 17.6089 - PSNR: 18.1473\n",
      "5000/5000 [==============================] - 104s 21ms/step\n",
      "5000/5000 [==============================] - 104s 21ms/step\n",
      "[CV]  batch_size=4, learning_rate=0.005384601561294807, optimizer=<class 'keras.optimizers.Adadelta'>, score=-17.553614336776732, total=63.6min\n",
      "[CV] batch_size=4, learning_rate=0.005384601561294807, optimizer=<class 'keras.optimizers.Adadelta'> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 65.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 376s 75ms/step - loss: 30.7995 - PSNR: 15.4675\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 372s 74ms/step - loss: 24.1154 - PSNR: 16.6319\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 373s 75ms/step - loss: 22.6893 - PSNR: 16.8995\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 367s 73ms/step - loss: 21.5335 - PSNR: 17.1550\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 368s 74ms/step - loss: 20.5877 - PSNR: 17.4006\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 370s 74ms/step - loss: 19.7746 - PSNR: 17.6031\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 373s 75ms/step - loss: 19.1368 - PSNR: 17.7566\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 373s 75ms/step - loss: 18.5913 - PSNR: 17.8962\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 367s 73ms/step - loss: 18.1487 - PSNR: 18.0016\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 369s 74ms/step - loss: 17.7031 - PSNR: 18.1103\n",
      "5000/5000 [==============================] - 105s 21ms/step\n",
      "5000/5000 [==============================] - 104s 21ms/step\n",
      "[CV]  batch_size=4, learning_rate=0.005384601561294807, optimizer=<class 'keras.optimizers.Adadelta'>, score=-17.395062197875976, total=63.6min\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "[CV] batch_size=16, learning_rate=0.05115349935179031, optimizer=<class 'keras.optimizers.Adadelta'> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 130.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 130.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 211s 42ms/step - loss: 33.0283 - PSNR: 14.9631\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 201s 40ms/step - loss: 25.0232 - PSNR: 16.2020\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 199s 40ms/step - loss: 22.5661 - PSNR: 16.7561\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 199s 40ms/step - loss: 20.8637 - PSNR: 17.1543\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 200s 40ms/step - loss: 19.6507 - PSNR: 17.5005\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 202s 40ms/step - loss: 18.7114 - PSNR: 17.7678\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 201s 40ms/step - loss: 17.7495 - PSNR: 18.0339\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 202s 40ms/step - loss: 17.2205 - PSNR: 18.1963\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 203s 41ms/step - loss: 16.4150 - PSNR: 18.4333\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 202s 40ms/step - loss: 16.0506 - PSNR: 18.5326\n",
      "5000/5000 [==============================] - 63s 13ms/step\n",
      "5000/5000 [==============================] - 63s 13ms/step\n",
      "[CV]  batch_size=16, learning_rate=0.05115349935179031, optimizer=<class 'keras.optimizers.Adadelta'>, score=-16.714290786743163, total=34.8min\n",
      "[CV] batch_size=16, learning_rate=0.05115349935179031, optimizer=<class 'keras.optimizers.Adadelta'> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 35.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 204s 41ms/step - loss: 32.9921 - PSNR: 14.9311\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 201s 40ms/step - loss: 24.9142 - PSNR: 16.1919\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 202s 40ms/step - loss: 22.6934 - PSNR: 16.7127\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 202s 40ms/step - loss: 20.7778 - PSNR: 17.1992\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 201s 40ms/step - loss: 19.4648 - PSNR: 17.5745\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 202s 40ms/step - loss: 18.2503 - PSNR: 17.8782\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 206s 41ms/step - loss: 17.4120 - PSNR: 18.1092\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 204s 41ms/step - loss: 16.6010 - PSNR: 18.3481\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 202s 40ms/step - loss: 16.1405 - PSNR: 18.4826\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 201s 40ms/step - loss: 15.6820 - PSNR: 18.6204\n",
      "5000/5000 [==============================] - 63s 13ms/step\n",
      "5000/5000 [==============================] - 63s 13ms/step\n",
      "[CV]  batch_size=16, learning_rate=0.05115349935179031, optimizer=<class 'keras.optimizers.Adadelta'>, score=-15.118521627807617, total=34.9min\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "[CV] batch_size=4, learning_rate=0.0015094143086495807, optimizer=<class 'keras.optimizers.Adadelta'> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 71.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 71.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 375s 75ms/step - loss: 36.5279 - PSNR: 14.6472\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 371s 74ms/step - loss: 27.7304 - PSNR: 16.1140\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 373s 75ms/step - loss: 26.2730 - PSNR: 16.3854\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 370s 74ms/step - loss: 25.4109 - PSNR: 16.5146\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 372s 74ms/step - loss: 24.7614 - PSNR: 16.6074\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 373s 75ms/step - loss: 24.2217 - PSNR: 16.6925\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 372s 74ms/step - loss: 23.7658 - PSNR: 16.7674\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 366s 73ms/step - loss: 23.2706 - PSNR: 16.8503\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 365s 73ms/step - loss: 22.7754 - PSNR: 16.9490\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 366s 73ms/step - loss: 22.3545 - PSNR: 17.0254\n",
      "5000/5000 [==============================] - 103s 21ms/step\n",
      "5000/5000 [==============================] - 102s 20ms/step\n",
      "[CV]  batch_size=4, learning_rate=0.0015094143086495807, optimizer=<class 'keras.optimizers.Adadelta'>, score=-21.928805897521972, total=63.5min\n",
      "[CV] batch_size=4, learning_rate=0.0015094143086495807, optimizer=<class 'keras.optimizers.Adadelta'> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 65.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 372s 74ms/step - loss: 35.7659 - PSNR: 14.7148\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 368s 74ms/step - loss: 27.6487 - PSNR: 16.0053\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 365s 73ms/step - loss: 26.4511 - PSNR: 16.2318\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 366s 73ms/step - loss: 25.7385 - PSNR: 16.3808\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 370s 74ms/step - loss: 25.1661 - PSNR: 16.4879\n",
      "Epoch 6/10\n",
      "1888/5000 [==========>...................] - ETA: 3:47 - loss: 24.8096 - PSNR: 16.5765Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "# log-uniform: understand as search over p = exp(x) by varying x\n",
    "# this is our parameter grid\n",
    "param_grid = {\n",
    "    'batch_size': [4, 8, 16],\n",
    "    'learning_rate': (1e-3, 1e-1, 'log-uniform'),\n",
    "    'activation' : ['relu', 'tanh'],\n",
    "    'optimizer': [SGD, RMSprop, Adam]\n",
    "}\n",
    "# Wrap Keras model inside sci-kit learn\n",
    "model = KerasRegressor(build_fn=get_unet_model, epochs=10)\n",
    "# Set up the optimiser to find the best parameters\n",
    "bayes = BayesSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    cv=2,\n",
    "    n_jobs=2,\n",
    "    verbose=10,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "bayes.fit(x, y)\n",
    "\n",
    "print('Best params achieve a test score of', bayes.score(x, y), ':')\n",
    "bayes.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QrGZDxNdgByl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bayes:\n",
      "{'activation': 'relu',\n",
      " 'batch_size': 8,\n",
      " 'learning_rate': 0.0028497323086495805,\n",
      " 'optimizer': <class 'keras.optimizers.Adam'>}\n"
     ]
    }
   ],
   "source": [
    "# model = KerasRegressor(build_fn=get_unet_model, epochs=20, batch_size=8)\n",
    "# model.fit(x=x, y=y)\n",
    "pprint.pprint(bayes.best_params_)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of Clean_target_training.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
