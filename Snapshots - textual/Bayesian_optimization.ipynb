{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Clean_target_training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "hvx74cPoAFdF",
        "colab_type": "code",
        "outputId": "8c98f9bb-5736-4ca9-9f6e-6346c5aa1f26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10951
        }
      },
      "cell_type": "code",
      "source": [
        "# When running on colab, run below commands\n",
        "!mkdir dataset\n",
        "%cd dataset\n",
        "!wget https://cv.snu.ac.kr/research/VDSR/train_data.zip\n",
        "!wget https://cv.snu.ac.kr/research/VDSR/test_data.zip\n",
        "!unzip train_data.zip\n",
        "!unzip test_data.zip\n",
        "%cd .."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/dataset\n",
            "--2018-12-03 18:00:35--  https://cv.snu.ac.kr/research/VDSR/train_data.zip\n",
            "Resolving cv.snu.ac.kr (cv.snu.ac.kr)... 147.46.116.247\n",
            "Connecting to cv.snu.ac.kr (cv.snu.ac.kr)|147.46.116.247|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41628740 (40M) [application/zip]\n",
            "Saving to: ‘train_data.zip’\n",
            "\n",
            "train_data.zip      100%[===================>]  39.70M   846KB/s    in 39s     \n",
            "\n",
            "2018-12-03 18:01:14 (1.02 MB/s) - ‘train_data.zip’ saved [41628740/41628740]\n",
            "\n",
            "--2018-12-03 18:01:15--  https://cv.snu.ac.kr/research/VDSR/test_data.zip\n",
            "Resolving cv.snu.ac.kr (cv.snu.ac.kr)... 147.46.116.247\n",
            "Connecting to cv.snu.ac.kr (cv.snu.ac.kr)|147.46.116.247|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62972123 (60M) [application/zip]\n",
            "Saving to: ‘test_data.zip’\n",
            "\n",
            "test_data.zip       100%[===================>]  60.05M   896KB/s    in 39s     \n",
            "\n",
            "2018-12-03 18:01:54 (1.55 MB/s) - ‘test_data.zip’ saved [62972123/62972123]\n",
            "\n",
            "Archive:  train_data.zip\n",
            "   creating: 291/\n",
            "  inflating: 291/000t1.bmp           \n",
            "  inflating: 291/000t10.bmp          \n",
            "  inflating: 291/000t11.bmp          \n",
            "  inflating: 291/000t12.bmp          \n",
            "  inflating: 291/000t13.bmp          \n",
            "  inflating: 291/000t14.bmp          \n",
            "  inflating: 291/000t15.bmp          \n",
            "  inflating: 291/000t16.bmp          \n",
            "  inflating: 291/000t17.bmp          \n",
            "  inflating: 291/000t18.bmp          \n",
            "  inflating: 291/000t19.bmp          \n",
            "  inflating: 291/000t2.bmp           \n",
            "  inflating: 291/000t20.bmp          \n",
            "  inflating: 291/000t21.bmp          \n",
            "  inflating: 291/000t22.bmp          \n",
            "  inflating: 291/000t23.bmp          \n",
            "  inflating: 291/000t24.bmp          \n",
            "  inflating: 291/000t25.bmp          \n",
            "  inflating: 291/000t26.bmp          \n",
            "  inflating: 291/000t27.bmp          \n",
            "  inflating: 291/000t28.bmp          \n",
            "  inflating: 291/000t29.bmp          \n",
            "  inflating: 291/000t3.bmp           \n",
            "  inflating: 291/000t30.bmp          \n",
            "  inflating: 291/000t31.bmp          \n",
            "  inflating: 291/000t32.bmp          \n",
            "  inflating: 291/000t33.bmp          \n",
            "  inflating: 291/000t34.bmp          \n",
            "  inflating: 291/000t35.bmp          \n",
            "  inflating: 291/000t36.bmp          \n",
            "  inflating: 291/000t37.bmp          \n",
            "  inflating: 291/000t38.bmp          \n",
            "  inflating: 291/000t39.bmp          \n",
            "  inflating: 291/000t4.bmp           \n",
            "  inflating: 291/000t40.bmp          \n",
            "  inflating: 291/000t42.bmp          \n",
            "  inflating: 291/000t43.bmp          \n",
            "  inflating: 291/000t44.bmp          \n",
            "  inflating: 291/000t45.bmp          \n",
            "  inflating: 291/000t46.bmp          \n",
            "  inflating: 291/000t47.bmp          \n",
            "  inflating: 291/000t48.bmp          \n",
            "  inflating: 291/000t49.bmp          \n",
            "  inflating: 291/000t5.bmp           \n",
            "  inflating: 291/000t50.bmp          \n",
            "  inflating: 291/000t51.bmp          \n",
            "  inflating: 291/000t52.bmp          \n",
            "  inflating: 291/000t53.bmp          \n",
            "  inflating: 291/000t54.bmp          \n",
            "  inflating: 291/000t55.bmp          \n",
            "  inflating: 291/000t56.bmp          \n",
            "  inflating: 291/000t57.bmp          \n",
            "  inflating: 291/000t58.bmp          \n",
            "  inflating: 291/000t59.bmp          \n",
            "  inflating: 291/000t6.bmp           \n",
            "  inflating: 291/000t60.bmp          \n",
            "  inflating: 291/000t61.bmp          \n",
            "  inflating: 291/000t62.bmp          \n",
            "  inflating: 291/000t63.bmp          \n",
            "  inflating: 291/000t64.bmp          \n",
            "  inflating: 291/000t65.bmp          \n",
            "  inflating: 291/000t66.bmp          \n",
            "  inflating: 291/000t7.bmp           \n",
            "  inflating: 291/000t8.bmp           \n",
            "  inflating: 291/000t9.bmp           \n",
            "  inflating: 291/000tt1.bmp          \n",
            "  inflating: 291/000tt10.bmp         \n",
            "  inflating: 291/000tt12.bmp         \n",
            "  inflating: 291/000tt13.bmp         \n",
            "  inflating: 291/000tt14.bmp         \n",
            "  inflating: 291/000tt15.bmp         \n",
            "  inflating: 291/000tt16.bmp         \n",
            "  inflating: 291/000tt17.bmp         \n",
            "  inflating: 291/000tt18.bmp         \n",
            "  inflating: 291/000tt19.bmp         \n",
            "  inflating: 291/000tt2.bmp          \n",
            "  inflating: 291/000tt20.bmp         \n",
            "  inflating: 291/000tt21.bmp         \n",
            "  inflating: 291/000tt22.bmp         \n",
            "  inflating: 291/000tt23.bmp         \n",
            "  inflating: 291/000tt24.bmp         \n",
            "  inflating: 291/000tt25.bmp         \n",
            "  inflating: 291/000tt26.bmp         \n",
            "  inflating: 291/000tt27.bmp         \n",
            "  inflating: 291/000tt3.bmp          \n",
            "  inflating: 291/000tt4.bmp          \n",
            "  inflating: 291/000tt5.bmp          \n",
            "  inflating: 291/000tt6.bmp          \n",
            "  inflating: 291/000tt7.bmp          \n",
            "  inflating: 291/000tt8.bmp          \n",
            "  inflating: 291/000tt9.bmp          \n",
            "  inflating: 291/100075.jpg          \n",
            "  inflating: 291/100080.jpg          \n",
            "  inflating: 291/100098.jpg          \n",
            "  inflating: 291/103041.jpg          \n",
            "  inflating: 291/104022.jpg          \n",
            "  inflating: 291/105019.jpg          \n",
            "  inflating: 291/105053.jpg          \n",
            "  inflating: 291/106020.jpg          \n",
            "  inflating: 291/106025.jpg          \n",
            "  inflating: 291/108041.jpg          \n",
            "  inflating: 291/108073.jpg          \n",
            "  inflating: 291/109034.jpg          \n",
            "  inflating: 291/112082.jpg          \n",
            "  inflating: 291/113009.jpg          \n",
            "  inflating: 291/113016.jpg          \n",
            "  inflating: 291/113044.jpg          \n",
            "  inflating: 291/117054.jpg          \n",
            "  inflating: 291/118020.jpg          \n",
            "  inflating: 291/118035.jpg          \n",
            "  inflating: 291/12003.jpg           \n",
            "  inflating: 291/12074.jpg           \n",
            "  inflating: 291/122048.jpg          \n",
            "  inflating: 291/124084.jpg          \n",
            "  inflating: 291/126039.jpg          \n",
            "  inflating: 291/130034.jpg          \n",
            "  inflating: 291/134008.jpg          \n",
            "  inflating: 291/134052.jpg          \n",
            "  inflating: 291/135037.jpg          \n",
            "  inflating: 291/135069.jpg          \n",
            "  inflating: 291/138032.jpg          \n",
            "  inflating: 291/138078.jpg          \n",
            "  inflating: 291/140055.jpg          \n",
            "  inflating: 291/140075.jpg          \n",
            "  inflating: 291/144067.jpg          \n",
            "  inflating: 291/145014.jpg          \n",
            "  inflating: 291/145053.jpg          \n",
            "  inflating: 291/147021.jpg          \n",
            "  inflating: 291/147062.jpg          \n",
            "  inflating: 291/15004.jpg           \n",
            "  inflating: 291/15088.jpg           \n",
            "  inflating: 291/151087.jpg          \n",
            "  inflating: 291/153077.jpg          \n",
            "  inflating: 291/153093.jpg          \n",
            "  inflating: 291/155060.jpg          \n",
            "  inflating: 291/156079.jpg          \n",
            "  inflating: 291/157036.jpg          \n",
            "  inflating: 291/159029.jpg          \n",
            "  inflating: 291/159045.jpg          \n",
            "  inflating: 291/159091.jpg          \n",
            "  inflating: 291/16052.jpg           \n",
            "  inflating: 291/161062.jpg          \n",
            "  inflating: 291/163014.jpg          \n",
            "  inflating: 291/163062.jpg          \n",
            "  inflating: 291/164074.jpg          \n",
            "  inflating: 291/166081.jpg          \n",
            "  inflating: 291/169012.jpg          \n",
            "  inflating: 291/170054.jpg          \n",
            "  inflating: 291/172032.jpg          \n",
            "  inflating: 291/173036.jpg          \n",
            "  inflating: 291/176019.jpg          \n",
            "  inflating: 291/176035.jpg          \n",
            "  inflating: 291/176039.jpg          \n",
            "  inflating: 291/178054.jpg          \n",
            "  inflating: 291/181018.jpg          \n",
            "  inflating: 291/181079.jpg          \n",
            "  inflating: 291/181091.jpg          \n",
            "  inflating: 291/183055.jpg          \n",
            "  inflating: 291/183087.jpg          \n",
            "  inflating: 291/187003.jpg          \n",
            "  inflating: 291/187029.jpg          \n",
            "  inflating: 291/187039.jpg          \n",
            "  inflating: 291/187071.jpg          \n",
            "  inflating: 291/187083.jpg          \n",
            "  inflating: 291/188005.jpg          \n",
            "  inflating: 291/188063.jpg          \n",
            "  inflating: 291/188091.jpg          \n",
            "  inflating: 291/189003.jpg          \n",
            "  inflating: 291/189011.jpg          \n",
            "  inflating: 291/196015.jpg          \n",
            "  inflating: 291/198004.jpg          \n",
            "  inflating: 291/198023.jpg          \n",
            "  inflating: 291/198054.jpg          \n",
            "  inflating: 291/20008.jpg           \n",
            "  inflating: 291/202012.jpg          \n",
            "  inflating: 291/207056.jpg          \n",
            "  inflating: 291/209070.jpg          \n",
            "  inflating: 291/2092.jpg            \n",
            "  inflating: 291/216041.jpg          \n",
            "  inflating: 291/216053.jpg          \n",
            "  inflating: 291/216066.jpg          \n",
            "  inflating: 291/22013.jpg           \n",
            "  inflating: 291/22090.jpg           \n",
            "  inflating: 291/22093.jpg           \n",
            "  inflating: 291/225017.jpg          \n",
            "  inflating: 291/227040.jpg          \n",
            "  inflating: 291/227046.jpg          \n",
            "  inflating: 291/23025.jpg           \n",
            "  inflating: 291/23080.jpg           \n",
            "  inflating: 291/23084.jpg           \n",
            "  inflating: 291/231015.jpg          \n",
            "  inflating: 291/232038.jpg          \n",
            "  inflating: 291/236017.jpg          \n",
            "  inflating: 291/238011.jpg          \n",
            "  inflating: 291/239007.jpg          \n",
            "  inflating: 291/239096.jpg          \n",
            "  inflating: 291/24004.jpg           \n",
            "  inflating: 291/24063.jpg           \n",
            "  inflating: 291/242078.jpg          \n",
            "  inflating: 291/245051.jpg          \n",
            "  inflating: 291/246016.jpg          \n",
            "  inflating: 291/246053.jpg          \n",
            "  inflating: 291/247085.jpg          \n",
            "  inflating: 291/249061.jpg          \n",
            "  inflating: 291/249087.jpg          \n",
            "  inflating: 291/25098.jpg           \n",
            "  inflating: 291/253036.jpg          \n",
            "  inflating: 291/254033.jpg          \n",
            "  inflating: 291/254054.jpg          \n",
            "  inflating: 291/260081.jpg          \n",
            "  inflating: 291/26031.jpg           \n",
            "  inflating: 291/268002.jpg          \n",
            "  inflating: 291/27059.jpg           \n",
            "  inflating: 291/271008.jpg          \n",
            "  inflating: 291/271031.jpg          \n",
            "  inflating: 291/274007.jpg          \n",
            "  inflating: 291/277095.jpg          \n",
            "  inflating: 291/28075.jpg           \n",
            "  inflating: 291/28096.jpg           \n",
            "  inflating: 291/285036.jpg          \n",
            "  inflating: 291/286092.jpg          \n",
            "  inflating: 291/292066.jpg          \n",
            "  inflating: 291/293029.jpg          \n",
            "  inflating: 291/299091.jpg          \n",
            "  inflating: 291/301007.jpg          \n",
            "  inflating: 291/302003.jpg          \n",
            "  inflating: 291/309004.jpg          \n",
            "  inflating: 291/310007.jpg          \n",
            "  inflating: 291/311068.jpg          \n",
            "  inflating: 291/311081.jpg          \n",
            "  inflating: 291/314016.jpg          \n",
            "  inflating: 291/317080.jpg          \n",
            "  inflating: 291/323016.jpg          \n",
            "  inflating: 291/326038.jpg          \n",
            "  inflating: 291/33066.jpg           \n",
            "  inflating: 291/35008.jpg           \n",
            "  inflating: 291/35010.jpg           \n",
            "  inflating: 291/35058.jpg           \n",
            "  inflating: 291/35070.jpg           \n",
            "  inflating: 291/35091.jpg           \n",
            "  inflating: 291/353013.jpg          \n",
            "  inflating: 291/361084.jpg          \n",
            "  inflating: 291/365025.jpg          \n",
            "  inflating: 291/365073.jpg          \n",
            "  inflating: 291/368016.jpg          \n",
            "  inflating: 291/368078.jpg          \n",
            "  inflating: 291/370036.jpg          \n",
            "  inflating: 291/372047.jpg          \n",
            "  inflating: 291/374020.jpg          \n",
            "  inflating: 291/374067.jpg          \n",
            "  inflating: 291/376001.jpg          \n",
            "  inflating: 291/376020.jpg          \n",
            "  inflating: 291/385028.jpg          \n",
            "  inflating: 291/388016.jpg          \n",
            "  inflating: 291/41004.jpg           \n",
            "  inflating: 291/41025.jpg           \n",
            "  inflating: 291/42044.jpg           \n",
            "  inflating: 291/42078.jpg           \n",
            "  inflating: 291/43070.jpg           \n",
            "  inflating: 291/43083.jpg           \n",
            "  inflating: 291/45077.jpg           \n",
            "  inflating: 291/46076.jpg           \n",
            "  inflating: 291/48055.jpg           \n",
            "  inflating: 291/54005.jpg           \n",
            "  inflating: 291/55067.jpg           \n",
            "  inflating: 291/55075.jpg           \n",
            "  inflating: 291/56028.jpg           \n",
            "  inflating: 291/59078.jpg           \n",
            "  inflating: 291/60079.jpg           \n",
            "  inflating: 291/61060.jpg           \n",
            "  inflating: 291/61086.jpg           \n",
            "  inflating: 291/65010.jpg           \n",
            "  inflating: 291/65019.jpg           \n",
            "  inflating: 291/65074.jpg           \n",
            "  inflating: 291/65132.jpg           \n",
            "  inflating: 291/66039.jpg           \n",
            "  inflating: 291/66075.jpg           \n",
            "  inflating: 291/67079.jpg           \n",
            "  inflating: 291/68077.jpg           \n",
            "  inflating: 291/71046.jpg           \n",
            "  inflating: 291/76002.jpg           \n",
            "  inflating: 291/78019.jpg           \n",
            "  inflating: 291/80099.jpg           \n",
            "  inflating: 291/8049.jpg            \n",
            "  inflating: 291/8143.jpg            \n",
            "  inflating: 291/87065.jpg           \n",
            "  inflating: 291/90076.jpg           \n",
            "  inflating: 291/92059.jpg           \n",
            "  inflating: 291/94079.jpg           \n",
            "  inflating: 291/95006.jpg           \n",
            "  inflating: 291/97017.jpg           \n",
            "   creating: 91/\n",
            "  inflating: 91/000t1.bmp            \n",
            "  inflating: 91/000t10.bmp           \n",
            "  inflating: 91/000t11.bmp           \n",
            "  inflating: 91/000t12.bmp           \n",
            "  inflating: 91/000t13.bmp           \n",
            "  inflating: 91/000t14.bmp           \n",
            "  inflating: 91/000t15.bmp           \n",
            "  inflating: 91/000t16.bmp           \n",
            "  inflating: 91/000t17.bmp           \n",
            "  inflating: 91/000t18.bmp           \n",
            "  inflating: 91/000t19.bmp           \n",
            "  inflating: 91/000t2.bmp            \n",
            "  inflating: 91/000t20.bmp           \n",
            "  inflating: 91/000t21.bmp           \n",
            "  inflating: 91/000t22.bmp           \n",
            "  inflating: 91/000t23.bmp           \n",
            "  inflating: 91/000t24.bmp           \n",
            "  inflating: 91/000t25.bmp           \n",
            "  inflating: 91/000t26.bmp           \n",
            "  inflating: 91/000t27.bmp           \n",
            "  inflating: 91/000t28.bmp           \n",
            "  inflating: 91/000t29.bmp           \n",
            "  inflating: 91/000t3.bmp            \n",
            "  inflating: 91/000t30.bmp           \n",
            "  inflating: 91/000t31.bmp           \n",
            "  inflating: 91/000t32.bmp           \n",
            "  inflating: 91/000t33.bmp           \n",
            "  inflating: 91/000t34.bmp           \n",
            "  inflating: 91/000t35.bmp           \n",
            "  inflating: 91/000t36.bmp           \n",
            "  inflating: 91/000t37.bmp           \n",
            "  inflating: 91/000t38.bmp           \n",
            "  inflating: 91/000t39.bmp           \n",
            "  inflating: 91/000t4.bmp            \n",
            "  inflating: 91/000t40.bmp           \n",
            "  inflating: 91/000t42.bmp           \n",
            "  inflating: 91/000t43.bmp           \n",
            "  inflating: 91/000t44.bmp           \n",
            "  inflating: 91/000t45.bmp           \n",
            "  inflating: 91/000t46.bmp           \n",
            "  inflating: 91/000t47.bmp           \n",
            "  inflating: 91/000t48.bmp           \n",
            "  inflating: 91/000t49.bmp           \n",
            "  inflating: 91/000t5.bmp            \n",
            "  inflating: 91/000t50.bmp           \n",
            "  inflating: 91/000t51.bmp           \n",
            "  inflating: 91/000t52.bmp           \n",
            "  inflating: 91/000t53.bmp           \n",
            "  inflating: 91/000t54.bmp           \n",
            "  inflating: 91/000t55.bmp           \n",
            "  inflating: 91/000t56.bmp           \n",
            "  inflating: 91/000t57.bmp           \n",
            "  inflating: 91/000t58.bmp           \n",
            "  inflating: 91/000t59.bmp           \n",
            "  inflating: 91/000t6.bmp            \n",
            "  inflating: 91/000t60.bmp           \n",
            "  inflating: 91/000t61.bmp           \n",
            "  inflating: 91/000t62.bmp           \n",
            "  inflating: 91/000t63.bmp           \n",
            "  inflating: 91/000t64.bmp           \n",
            "  inflating: 91/000t65.bmp           \n",
            "  inflating: 91/000t66.bmp           \n",
            "  inflating: 91/000t7.bmp            \n",
            "  inflating: 91/000t8.bmp            \n",
            "  inflating: 91/000t9.bmp            \n",
            "  inflating: 91/000tt1.bmp           \n",
            "  inflating: 91/000tt10.bmp          \n",
            "  inflating: 91/000tt12.bmp          \n",
            "  inflating: 91/000tt13.bmp          \n",
            "  inflating: 91/000tt14.bmp          \n",
            "  inflating: 91/000tt15.bmp          \n",
            "  inflating: 91/000tt16.bmp          \n",
            "  inflating: 91/000tt17.bmp          \n",
            "  inflating: 91/000tt18.bmp          \n",
            "  inflating: 91/000tt19.bmp          \n",
            "  inflating: 91/000tt2.bmp           \n",
            "  inflating: 91/000tt20.bmp          \n",
            "  inflating: 91/000tt21.bmp          \n",
            "  inflating: 91/000tt22.bmp          \n",
            "  inflating: 91/000tt23.bmp          \n",
            "  inflating: 91/000tt24.bmp          \n",
            "  inflating: 91/000tt25.bmp          \n",
            "  inflating: 91/000tt26.bmp          \n",
            "  inflating: 91/000tt27.bmp          \n",
            "  inflating: 91/000tt3.bmp           \n",
            "  inflating: 91/000tt4.bmp           \n",
            "  inflating: 91/000tt5.bmp           \n",
            "  inflating: 91/000tt6.bmp           \n",
            "  inflating: 91/000tt7.bmp           \n",
            "  inflating: 91/000tt8.bmp           \n",
            "  inflating: 91/000tt9.bmp           \n",
            "Archive:  test_data.zip\n",
            "   creating: B100/\n",
            "  inflating: B100/101085.jpg         \n",
            "  inflating: B100/101087.jpg         \n",
            "  inflating: B100/102061.jpg         \n",
            "  inflating: B100/103070.jpg         \n",
            "  inflating: B100/105025.jpg         \n",
            "  inflating: B100/106024.jpg         \n",
            "  inflating: B100/108005.jpg         \n",
            "  inflating: B100/108070.jpg         \n",
            "  inflating: B100/108082.jpg         \n",
            "  inflating: B100/109053.jpg         \n",
            "  inflating: B100/119082.jpg         \n",
            "  inflating: B100/12084.jpg          \n",
            "  inflating: B100/123074.jpg         \n",
            "  inflating: B100/126007.jpg         \n",
            "  inflating: B100/130026.jpg         \n",
            "  inflating: B100/134035.jpg         \n",
            "  inflating: B100/14037.jpg          \n",
            "  inflating: B100/143090.jpg         \n",
            "  inflating: B100/145086.jpg         \n",
            "  inflating: B100/147091.jpg         \n",
            "  inflating: B100/148026.jpg         \n",
            "  inflating: B100/148089.jpg         \n",
            "  inflating: B100/156065.jpg         \n",
            "  inflating: B100/157055.jpg         \n",
            "  inflating: B100/159008.jpg         \n",
            "  inflating: B100/160068.jpg         \n",
            "  inflating: B100/16077.jpg          \n",
            "  inflating: B100/163085.jpg         \n",
            "  inflating: B100/167062.jpg         \n",
            "  inflating: B100/167083.jpg         \n",
            "  inflating: B100/170057.jpg         \n",
            "  inflating: B100/175032.jpg         \n",
            "  inflating: B100/175043.jpg         \n",
            "  inflating: B100/182053.jpg         \n",
            "  inflating: B100/189080.jpg         \n",
            "  inflating: B100/19021.jpg          \n",
            "  inflating: B100/196073.jpg         \n",
            "  inflating: B100/197017.jpg         \n",
            "  inflating: B100/208001.jpg         \n",
            "  inflating: B100/210088.jpg         \n",
            "  inflating: B100/21077.jpg          \n",
            "  inflating: B100/216081.jpg         \n",
            "  inflating: B100/219090.jpg         \n",
            "  inflating: B100/220075.jpg         \n",
            "  inflating: B100/223061.jpg         \n",
            "  inflating: B100/227092.jpg         \n",
            "  inflating: B100/229036.jpg         \n",
            "  inflating: B100/236037.jpg         \n",
            "  inflating: B100/24077.jpg          \n",
            "  inflating: B100/241004.jpg         \n",
            "  inflating: B100/241048.jpg         \n",
            "  inflating: B100/253027.jpg         \n",
            "  inflating: B100/253055.jpg         \n",
            "  inflating: B100/260058.jpg         \n",
            "  inflating: B100/271035.jpg         \n",
            "  inflating: B100/285079.jpg         \n",
            "  inflating: B100/291000.jpg         \n",
            "  inflating: B100/295087.jpg         \n",
            "  inflating: B100/296007.jpg         \n",
            "  inflating: B100/296059.jpg         \n",
            "  inflating: B100/299086.jpg         \n",
            "  inflating: B100/300091.jpg         \n",
            "  inflating: B100/302008.jpg         \n",
            "  inflating: B100/304034.jpg         \n",
            "  inflating: B100/304074.jpg         \n",
            "  inflating: B100/306005.jpg         \n",
            "  inflating: B100/3096.jpg           \n",
            "  inflating: B100/33039.jpg          \n",
            "  inflating: B100/351093.jpg         \n",
            "  inflating: B100/361010.jpg         \n",
            "  inflating: B100/37073.jpg          \n",
            "  inflating: B100/376043.jpg         \n",
            "  inflating: B100/38082.jpg          \n",
            "  inflating: B100/38092.jpg          \n",
            "  inflating: B100/385039.jpg         \n",
            "  inflating: B100/41033.jpg          \n",
            "  inflating: B100/41069.jpg          \n",
            "  inflating: B100/42012.jpg          \n",
            "  inflating: B100/42049.jpg          \n",
            "  inflating: B100/43074.jpg          \n",
            "  inflating: B100/45096.jpg          \n",
            "  inflating: B100/54082.jpg          \n",
            "  inflating: B100/55073.jpg          \n",
            "  inflating: B100/58060.jpg          \n",
            "  inflating: B100/62096.jpg          \n",
            "  inflating: B100/65033.jpg          \n",
            "  inflating: B100/66053.jpg          \n",
            "  inflating: B100/69015.jpg          \n",
            "  inflating: B100/69020.jpg          \n",
            "  inflating: B100/69040.jpg          \n",
            "  inflating: B100/76053.jpg          \n",
            "  inflating: B100/78004.jpg          \n",
            "  inflating: B100/8023.jpg           \n",
            "  inflating: B100/85048.jpg          \n",
            "  inflating: B100/86000.jpg          \n",
            "  inflating: B100/86016.jpg          \n",
            "  inflating: B100/86068.jpg          \n",
            "  inflating: B100/87046.jpg          \n",
            "  inflating: B100/89072.jpg          \n",
            "  inflating: B100/97033.jpg          \n",
            "   creating: Set14/\n",
            "  inflating: Set14/baboon.bmp        \n",
            "  inflating: Set14/barbara.bmp       \n",
            "  inflating: Set14/bridge.bmp        \n",
            "  inflating: Set14/coastguard.bmp    \n",
            "  inflating: Set14/comic.bmp         \n",
            "  inflating: Set14/face.bmp          \n",
            "  inflating: Set14/flowers.bmp       \n",
            "  inflating: Set14/foreman.bmp       \n",
            "  inflating: Set14/lenna.bmp         \n",
            "  inflating: Set14/man.bmp           \n",
            "  inflating: Set14/monarch.bmp       \n",
            "  inflating: Set14/pepper.bmp        \n",
            "  inflating: Set14/ppt3.bmp          \n",
            "  inflating: Set14/zebra.bmp         \n",
            "   creating: Set5/\n",
            "  inflating: Set5/baby_GT.bmp        \n",
            "  inflating: Set5/bird_GT.bmp        \n",
            "  inflating: Set5/butterfly_GT.bmp   \n",
            "  inflating: Set5/head_GT.bmp        \n",
            "  inflating: Set5/woman_GT.bmp       \n",
            "   creating: Urban100/\n",
            "  inflating: Urban100/img001.jpg     \n",
            "  inflating: Urban100/img002.jpg     \n",
            "  inflating: Urban100/img003.jpg     \n",
            "  inflating: Urban100/img004.jpg     \n",
            "  inflating: Urban100/img005.jpg     \n",
            "  inflating: Urban100/img006.jpg     \n",
            "  inflating: Urban100/img007.jpg     \n",
            "  inflating: Urban100/img008.jpg     \n",
            "  inflating: Urban100/img009.jpg     \n",
            "  inflating: Urban100/img010.jpg     \n",
            "  inflating: Urban100/img011.jpg     \n",
            "  inflating: Urban100/img012.jpg     \n",
            "  inflating: Urban100/img013.jpg     \n",
            "  inflating: Urban100/img014.jpg     \n",
            "  inflating: Urban100/img015.jpg     \n",
            "  inflating: Urban100/img016.jpg     \n",
            "  inflating: Urban100/img017.jpg     \n",
            "  inflating: Urban100/img018.jpg     \n",
            "  inflating: Urban100/img019.jpg     \n",
            "  inflating: Urban100/img020.jpg     \n",
            "  inflating: Urban100/img021.jpg     \n",
            "  inflating: Urban100/img022.jpg     \n",
            "  inflating: Urban100/img023.jpg     \n",
            "  inflating: Urban100/img024.jpg     \n",
            "  inflating: Urban100/img025.jpg     \n",
            "  inflating: Urban100/img026.jpg     \n",
            "  inflating: Urban100/img027.jpg     \n",
            "  inflating: Urban100/img028.jpg     \n",
            "  inflating: Urban100/img029.jpg     \n",
            "  inflating: Urban100/img030.jpg     \n",
            "  inflating: Urban100/img031.jpg     \n",
            "  inflating: Urban100/img032.jpg     \n",
            "  inflating: Urban100/img033.jpg     \n",
            "  inflating: Urban100/img034.jpg     \n",
            "  inflating: Urban100/img035.jpg     \n",
            "  inflating: Urban100/img036.jpg     \n",
            "  inflating: Urban100/img037.jpg     \n",
            "  inflating: Urban100/img038.jpg     \n",
            "  inflating: Urban100/img039.jpg     \n",
            "  inflating: Urban100/img040.jpg     \n",
            "  inflating: Urban100/img041.jpg     \n",
            "  inflating: Urban100/img042.jpg     \n",
            "  inflating: Urban100/img043.jpg     \n",
            "  inflating: Urban100/img044.jpg     \n",
            "  inflating: Urban100/img045.jpg     \n",
            "  inflating: Urban100/img046.jpg     \n",
            "  inflating: Urban100/img047.jpg     \n",
            "  inflating: Urban100/img048.jpg     \n",
            "  inflating: Urban100/img049.jpg     \n",
            "  inflating: Urban100/img050.jpg     \n",
            "  inflating: Urban100/img051.jpg     \n",
            "  inflating: Urban100/img052.jpg     \n",
            "  inflating: Urban100/img053.jpg     \n",
            "  inflating: Urban100/img054.jpg     \n",
            "  inflating: Urban100/img055.jpg     \n",
            "  inflating: Urban100/img056.jpg     \n",
            "  inflating: Urban100/img057.jpg     \n",
            "  inflating: Urban100/img058.jpg     \n",
            "  inflating: Urban100/img059.jpg     \n",
            "  inflating: Urban100/img060.jpg     \n",
            "  inflating: Urban100/img061.jpg     \n",
            "  inflating: Urban100/img062.jpg     \n",
            "  inflating: Urban100/img063.jpg     \n",
            "  inflating: Urban100/img064.jpg     \n",
            "  inflating: Urban100/img065.jpg     \n",
            "  inflating: Urban100/img066.jpg     \n",
            "  inflating: Urban100/img067.jpg     \n",
            "  inflating: Urban100/img068.jpg     \n",
            "  inflating: Urban100/img069.jpg     \n",
            "  inflating: Urban100/img070.jpg     \n",
            "  inflating: Urban100/img071.jpg     \n",
            "  inflating: Urban100/img072.jpg     \n",
            "  inflating: Urban100/img073.jpg     \n",
            "  inflating: Urban100/img074.jpg     \n",
            "  inflating: Urban100/img075.jpg     \n",
            "  inflating: Urban100/img076.jpg     \n",
            "  inflating: Urban100/img077.jpg     \n",
            "  inflating: Urban100/img078.jpg     \n",
            "  inflating: Urban100/img079.jpg     \n",
            "  inflating: Urban100/img080.jpg     \n",
            "  inflating: Urban100/img081.jpg     \n",
            "  inflating: Urban100/img082.jpg     \n",
            "  inflating: Urban100/img083.jpg     \n",
            "  inflating: Urban100/img084.jpg     \n",
            "  inflating: Urban100/img085.jpg     \n",
            "  inflating: Urban100/img086.jpg     \n",
            "  inflating: Urban100/img087.jpg     \n",
            "  inflating: Urban100/img088.jpg     \n",
            "  inflating: Urban100/img089.jpg     \n",
            "  inflating: Urban100/img090.jpg     \n",
            "  inflating: Urban100/img091.jpg     \n",
            "  inflating: Urban100/img092.jpg     \n",
            "  inflating: Urban100/img093.jpg     \n",
            "  inflating: Urban100/img094.jpg     \n",
            "  inflating: Urban100/img095.jpg     \n",
            "  inflating: Urban100/img096.jpg     \n",
            "  inflating: Urban100/img097.jpg     \n",
            "  inflating: Urban100/img098.jpg     \n",
            "  inflating: Urban100/img099.jpg     \n",
            "  inflating: Urban100/img100.jpg     \n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yIFszr6AAI46",
        "colab_type": "code",
        "outputId": "f97ab5cf-d41d-4e94-8eb5-c7742f04f5e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "cell_type": "code",
      "source": [
        "! pip install scikit-optimize"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/44/60f82c97d1caa98752c7da2c1681cab5c7a390a0fdd3a55fac672b321cac/scikit_optimize-0.5.2-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.14.6)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.19.2)\n",
            "Installing collected packages: scikit-optimize\n",
            "Successfully installed scikit-optimize-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wm4IzsJsAFjy",
        "colab_type": "code",
        "outputId": "a49858f6-d0e3-4a32-ce25-79061e7320e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#Importing the required packages\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "from keras.utils import Sequence\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Add, PReLU, Conv2DTranspose, Concatenate, MaxPooling2D, UpSampling2D, Dropout\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import Callback, LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
        "from keras import backend as K\n",
        "from keras.optimizers import *\n",
        "from keras.utils import Sequence\n",
        "# Using Keras Model in Scikit Learn\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "# include below until https://github.com/scikit-optimize/scikit-optimize/issues/718 is resolved\n",
        "class BayesSearchCV(BayesSearchCV):\n",
        "    def _run_search(self, x): raise BaseException('Use newer skopt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mkWdB-4kJYwG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_noise_model(noise_type=\"gaussian,0,50\"):\n",
        "    tokens = noise_type.split(sep=\",\")\n",
        "\n",
        "    if tokens[0] == \"gaussian\":\n",
        "        min_stddev = int(tokens[1])\n",
        "        max_stddev = int(tokens[2])\n",
        "\n",
        "        def gaussian_noise(img):\n",
        "            noise_img = img.astype(np.float)\n",
        "            stddev = np.random.uniform(min_stddev, max_stddev)\n",
        "            noise = np.random.randn(*img.shape) * stddev\n",
        "            noise_img += noise\n",
        "            noise_img = np.clip(noise_img, 0, 255).astype(np.uint8)\n",
        "            return noise_img\n",
        "\n",
        "        return gaussian_noise\n",
        "\n",
        "    elif tokens[0] == \"clean\":\n",
        "        return lambda img: img\n",
        "\n",
        "    elif tokens[0] == \"text\":\n",
        "\n",
        "        min_occupancy = int(tokens[1])\n",
        "        max_occupancy = int(tokens[2])\n",
        "\n",
        "        def add_text(img):\n",
        "\n",
        "            img = img.copy()\n",
        "            h, w, _ = img.shape\n",
        "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "            img_for_cnt = np.zeros((h, w), np.uint8)\n",
        "            occupancy = np.random.uniform(min_occupancy, max_occupancy)\n",
        "\n",
        "            while True:\n",
        "                n = random.randint(5, 10)\n",
        "                random_str = ''.join([random.choice(string.ascii_letters + string.digits) for i in range(n)])\n",
        "                font_scale = np.random.uniform(0.5, 1)\n",
        "                thickness = random.randint(1, 3)\n",
        "                (fw, fh), baseline = cv2.getTextSize(random_str, font, font_scale, thickness)\n",
        "                x = random.randint(0, max(0, w - 1 - fw))\n",
        "                y = random.randint(fh, h - 1 - baseline)\n",
        "                color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
        "                cv2.putText(img, random_str, (x, y), font, font_scale, color, thickness)\n",
        "                cv2.putText(img_for_cnt, random_str, (x, y), font, font_scale, 255, thickness)\n",
        "\n",
        "                if (img_for_cnt > 0).sum() > h * w * occupancy / 100:\n",
        "                    break\n",
        "            return img\n",
        "\n",
        "        return add_text\n",
        "\n",
        "\n",
        "    elif tokens[0] == \"impulse\":\n",
        "\n",
        "        min_occupancy = int(tokens[1])\n",
        "        max_occupancy = int(tokens[2])\n",
        "\n",
        "        def add_impulse_noise(img):\n",
        "            occupancy = np.random.uniform(min_occupancy, max_occupancy)\n",
        "            mask = np.random.binomial(size=img.shape, n=1, p=occupancy / 100)\n",
        "            noise = np.random.randint(256, size=img.shape)\n",
        "            img = img * (1 - mask) + noise * mask\n",
        "            return img.astype(np.uint8)\n",
        "        return add_impulse_noise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ovYc7MH-JYwJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NoisyImageGenerator(Sequence):\n",
        "  \n",
        "  def __init__(self, image_dir, source_noise_model, target_noise_model, batch_size=10000, image_size=128):\n",
        "\n",
        "    self.image_paths = list(Path(image_dir).glob(\"*.jpg\"))\n",
        "    self.source_noise_model = source_noise_model\n",
        "    self.target_noise_model = target_noise_model\n",
        "    self.image_num = len(self.image_paths)\n",
        "    self.batch_size = batch_size\n",
        "    self.image_size = image_size\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    return self.image_num // self.batch_size\n",
        "\n",
        "  def __getitem__(self):\n",
        "\n",
        "    batch_size = self.batch_size\n",
        "    image_size = self.image_size\n",
        "    x = np.zeros((batch_size, image_size, image_size, 3), dtype=np.uint8)\n",
        "    y = np.zeros((batch_size, image_size, image_size, 3), dtype=np.uint8)\n",
        "    sample_id = 0\n",
        "\n",
        "    while True:\n",
        "      \n",
        "      image_path = random.choice(self.image_paths)\n",
        "      image = cv2.imread(str(image_path))\n",
        "      h, w, _ = image.shape\n",
        "\n",
        "      if h >= image_size and w >= image_size:\n",
        "          h, w, _ = image.shape\n",
        "          i = np.random.randint(h - image_size + 1)\n",
        "          j = np.random.randint(w - image_size + 1)\n",
        "          clean_patch = image[i:i + image_size, j:j + image_size]\n",
        "          x[sample_id] = self.source_noise_model(clean_patch)\n",
        "          y[sample_id] = self.target_noise_model(clean_patch)\n",
        "\n",
        "          sample_id += 1\n",
        "\n",
        "          if sample_id == batch_size:\n",
        "              return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-wGPxtoeJYwL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Generator(Sequence):\n",
        "    def __init__(self, image_dir, val_noise_model):\n",
        "\n",
        "        image_paths = list(Path(image_dir).glob(\"*.*\"))\n",
        "        self.image_num = len(image_paths)\n",
        "        self.data = []\n",
        "\n",
        "        for image_path in image_paths:\n",
        "            y = cv2.imread(str(image_path))\n",
        "            h, w, _ = y.shape\n",
        "            y = y[:(h // 16) * 16, :(w // 16) * 16]  # for stride (maximum 16)\n",
        "            x = val_noise_model(y)\n",
        "            self.data.append(x)\n",
        "            #print (x.shape)\n",
        "            #print(y.shape)\n",
        "            #self.data.append([np.expand_dims(x, axis=0), np.expand_dims(y, axis=0)])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.image_num\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "    \n",
        "    def get_data(self):\n",
        "        return self.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hJbjChdOJYwN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "source_noise_model = get_noise_model(\"text,0,50\")\n",
        "target_noise_model = get_noise_model( \"text,0,0\")\n",
        "val_noise_model = get_noise_model(\"text,0,50\")\n",
        "\n",
        "image_dir = \"dataset/291\"\n",
        "test_dir =  \"dataset/Set14\"\n",
        "\n",
        "# batch_size = 4\n",
        "# learning_rate = 0.003\n",
        "\n",
        "# noisy_generator = NoisyImageGenerator(image_dir, source_noise_model, target_noise_model, batch_size=batch_size,\n",
        "#                               image_size=64)\n",
        "# val_generator = ValGenerator(test_dir, val_noise_model)\n",
        "x, y = np.array(NoisyImageGenerator(image_dir, source_noise_model, target_noise_model).__getitem__())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QpyJ9dcEJYwQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tf_log10(x):\n",
        "    numerator = tf.log(x)\n",
        "    denominator = tf.log(tf.constant(10, dtype=numerator.dtype))\n",
        "    return numerator / denominator\n",
        "\n",
        "def PSNR(y_true, y_pred):\n",
        "    max_pixel = 255.0\n",
        "    y_pred = K.clip(y_pred, 0.0, 255.0)\n",
        "    return 10.0 * tf_log10((max_pixel ** 2) / (K.mean(K.square(y_pred - y_true))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Xqw3GI3CJYwT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_unet_model(activation, learning_rate, optimizer, input_channel_num=3, out_ch=3, start_ch=64, depth=4, \n",
        "                   inc_rate=2., dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n",
        "    def _conv_block(m, dim, acti, bn, res, do=0):\n",
        "        n = Conv2D(dim, 3, activation=acti, padding='same')(m)\n",
        "        n = BatchNormalization()(n) if bn else n\n",
        "        n = Dropout(do)(n) if do else n\n",
        "        n = Conv2D(dim, 3, activation=acti, padding='same')(n)\n",
        "        n = BatchNormalization()(n) if bn else n\n",
        "\n",
        "        return Concatenate()([m, n]) if res else n\n",
        "\n",
        "    def _level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n",
        "        if depth > 0:\n",
        "            n = _conv_block(m, dim, acti, bn, res)\n",
        "            m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
        "            m = _level_block(m, int(inc * dim), depth - 1, inc, acti, do, bn, mp, up, res)\n",
        "            if up:\n",
        "                m = UpSampling2D()(m)\n",
        "                m = Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
        "            else:\n",
        "                m = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n",
        "            n = Concatenate()([n, m])\n",
        "            m = _conv_block(n, dim, acti, bn, res)\n",
        "        else:\n",
        "            m = _conv_block(m, dim, acti, bn, res, do)\n",
        "\n",
        "        return m\n",
        "\n",
        "    i = Input(shape=(None, None, input_channel_num))\n",
        "    o = _level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
        "    o = Conv2D(out_ch, 1)(o)\n",
        "    model = Model(inputs=i, outputs=o)\n",
        "    \n",
        "    model.compile(optimizer=optimizer(lr=learning_rate), loss=\"mae\", metrics=[PSNR])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BgZw94Aw_-jI",
        "colab_type": "code",
        "outputId": "e169ba87-13a6-4e3b-c452-5c3cce6322bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5290
        }
      },
      "cell_type": "code",
      "source": [
        "# log-uniform: understand as search over p = exp(x) by varying x\n",
        "# this is our parameter grid\n",
        "param_grid = {\n",
        "    'batch_size': [4, 8, 16, 24],\n",
        "    'learning_rate': (5e-4, 1e-2, 'log-uniform'),\n",
        "    'activation' : ['relu', 'tanh', 'sigmoid'],\n",
        "    'optimizer': [SGD, RMSprop, Adagrad, Adadelta, Adam]\n",
        "}\n",
        "# Wrap Keras model inside sci-kit learn\n",
        "model = KerasRegressor(build_fn=get_unet_model, epochs=10)\n",
        "# set up our optimiser to find the best params in 30 searches\n",
        "bayes = BayesSearchCV(\n",
        "    model,\n",
        "    param_grid,\n",
        "    random_state=1234,\n",
        "    cv=2,\n",
        "    verbose=10,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "bayes.fit(x, y)\n",
        "\n",
        "print('Best params achieve a test score of', bayes.score(x, y), ':')\n",
        "bayes.best_params_"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] activation=tanh, batch_size=24, learning_rate=0.0008723017716958026, optimizer=<class 'keras.optimizers.Adam'> \n",
            "Epoch 1/10\n",
            "5000/5000 [==============================] - 207s 41ms/step - loss: 98.5787 - PSNR: 6.8264\n",
            "Epoch 2/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 88.1741 - PSNR: 7.5703\n",
            "Epoch 3/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 79.2287 - PSNR: 8.3058\n",
            "Epoch 4/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 71.6733 - PSNR: 9.0143\n",
            "Epoch 5/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 65.6230 - PSNR: 9.6842\n",
            "Epoch 6/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 60.9898 - PSNR: 10.2680\n",
            "Epoch 7/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 57.5652 - PSNR: 10.7716\n",
            "Epoch 8/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 55.1039 - PSNR: 11.1758\n",
            "Epoch 9/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 53.4209 - PSNR: 11.4923\n",
            "Epoch 10/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 52.3100 - PSNR: 11.7346\n",
            "5000/5000 [==============================] - 64s 13ms/step\n",
            "5000/5000 [==============================] - 64s 13ms/step\n",
            "[CV]  activation=tanh, batch_size=24, learning_rate=0.0008723017716958026, optimizer=<class 'keras.optimizers.Adam'>, score=-52.210239971923826, total=33.3min\n",
            "[CV] activation=tanh, batch_size=24, learning_rate=0.0008723017716958026, optimizer=<class 'keras.optimizers.Adam'> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 34.3min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5000/5000 [==============================] - 194s 39ms/step - loss: 98.4596 - PSNR: 6.8238\n",
            "Epoch 2/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 88.1620 - PSNR: 7.5674\n",
            "Epoch 3/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 79.2869 - PSNR: 8.2987\n",
            "Epoch 4/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 71.8009 - PSNR: 9.0212\n",
            "Epoch 5/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 65.8310 - PSNR: 9.6715\n",
            "Epoch 6/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 61.2864 - PSNR: 10.2520\n",
            "Epoch 7/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 57.9088 - PSNR: 10.7510\n",
            "Epoch 8/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 55.4734 - PSNR: 11.1598\n",
            "Epoch 9/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 53.7777 - PSNR: 11.4770\n",
            "Epoch 10/10\n",
            "5000/5000 [==============================] - 191s 38ms/step - loss: 52.6389 - PSNR: 11.7193\n",
            "5000/5000 [==============================] - 64s 13ms/step\n",
            "5000/5000 [==============================] - 64s 13ms/step\n",
            "[CV]  activation=tanh, batch_size=24, learning_rate=0.0008723017716958026, optimizer=<class 'keras.optimizers.Adam'>, score=-51.89427853393555, total=33.0min\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] activation=tanh, batch_size=8, learning_rate=0.004933466508216417, optimizer=<class 'keras.optimizers.Adam'> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 68.4min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 68.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5000/5000 [==============================] - 250s 50ms/step - loss: 61.0871 - PSNR: 10.7537\n",
            "Epoch 2/10\n",
            "5000/5000 [==============================] - 247s 49ms/step - loss: 50.7501 - PSNR: 12.2661\n",
            "Epoch 3/10\n",
            "5000/5000 [==============================] - 247s 49ms/step - loss: 50.7477 - PSNR: 12.2768\n",
            "Epoch 4/10\n",
            "5000/5000 [==============================] - 247s 49ms/step - loss: 50.7568 - PSNR: 12.2831\n",
            "Epoch 5/10\n",
            "5000/5000 [==============================] - 247s 49ms/step - loss: 50.7646 - PSNR: 12.2769\n",
            "Epoch 6/10\n",
            "5000/5000 [==============================] - 248s 50ms/step - loss: 50.7559 - PSNR: 12.2797\n",
            "Epoch 7/10\n",
            "5000/5000 [==============================] - 248s 50ms/step - loss: 50.7588 - PSNR: 12.2745\n",
            "Epoch 8/10\n",
            "5000/5000 [==============================] - 248s 50ms/step - loss: 50.7640 - PSNR: 12.2775\n",
            "Epoch 9/10\n",
            "5000/5000 [==============================] - 248s 50ms/step - loss: 50.7601 - PSNR: 12.2776\n",
            "Epoch 10/10\n",
            "5000/5000 [==============================] - 248s 50ms/step - loss: 50.7592 - PSNR: 12.2787\n",
            "5000/5000 [==============================] - 83s 17ms/step\n",
            "5000/5000 [==============================] - 83s 17ms/step\n",
            "[CV]  activation=tanh, batch_size=8, learning_rate=0.004933466508216417, optimizer=<class 'keras.optimizers.Adam'>, score=-50.95655955810547, total=42.7min\n",
            "[CV] activation=tanh, batch_size=8, learning_rate=0.004933466508216417, optimizer=<class 'keras.optimizers.Adam'> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 44.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5000/5000 [==============================] - 251s 50ms/step - loss: 61.3170 - PSNR: 10.7274\n",
            "Epoch 2/10\n",
            "5000/5000 [==============================] - 247s 49ms/step - loss: 50.9630 - PSNR: 12.2585\n",
            "Epoch 3/10\n",
            "5000/5000 [==============================] - 246s 49ms/step - loss: 50.9691 - PSNR: 12.2616\n",
            "Epoch 4/10\n",
            "5000/5000 [==============================] - 246s 49ms/step - loss: 50.9708 - PSNR: 12.2582\n",
            "Epoch 5/10\n",
            "5000/5000 [==============================] - 247s 49ms/step - loss: 50.9602 - PSNR: 12.2618\n",
            "Epoch 6/10\n",
            "5000/5000 [==============================] - 250s 50ms/step - loss: 50.9601 - PSNR: 12.2547\n",
            "Epoch 7/10\n",
            "5000/5000 [==============================] - 250s 50ms/step - loss: 50.9627 - PSNR: 12.2621\n",
            "Epoch 8/10\n",
            "5000/5000 [==============================] - 250s 50ms/step - loss: 50.9617 - PSNR: 12.2614\n",
            "Epoch 9/10\n",
            "5000/5000 [==============================] - 251s 50ms/step - loss: 50.9673 - PSNR: 12.2660\n",
            "Epoch 10/10\n",
            "5000/5000 [==============================] - 250s 50ms/step - loss: 50.9698 - PSNR: 12.2617\n",
            "5000/5000 [==============================] - 84s 17ms/step\n",
            "5000/5000 [==============================] - 84s 17ms/step\n",
            "[CV]  activation=tanh, batch_size=8, learning_rate=0.004933466508216417, optimizer=<class 'keras.optimizers.Adam'>, score=-50.74291151733399, total=42.9min\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] activation=relu, batch_size=24, learning_rate=0.0005902945472918615, optimizer=<class 'keras.optimizers.Adam'> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 88.4min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 88.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5000/5000 [==============================] - 193s 39ms/step - loss: 42.3812 - PSNR: 15.3738\n",
            "Epoch 2/10\n",
            "5000/5000 [==============================] - 188s 38ms/step - loss: 18.6487 - PSNR: 17.5035\n",
            "Epoch 3/10\n",
            "5000/5000 [==============================] - 188s 38ms/step - loss: 15.9632 - PSNR: 18.2725\n",
            "Epoch 4/10\n",
            "5000/5000 [==============================] - 188s 38ms/step - loss: 14.2971 - PSNR: 18.7876\n",
            "Epoch 5/10\n",
            "5000/5000 [==============================] - 188s 38ms/step - loss: 13.4183 - PSNR: 19.1236\n",
            "Epoch 6/10\n",
            "5000/5000 [==============================] - 188s 38ms/step - loss: 12.2852 - PSNR: 19.4245\n",
            "Epoch 7/10\n",
            "5000/5000 [==============================] - 188s 38ms/step - loss: 12.1186 - PSNR: 19.5186\n",
            "Epoch 8/10\n",
            "5000/5000 [==============================] - 189s 38ms/step - loss: 11.8027 - PSNR: 19.6492\n",
            "Epoch 9/10\n",
            "5000/5000 [==============================] - 189s 38ms/step - loss: 11.2752 - PSNR: 19.7988\n",
            "Epoch 10/10\n",
            "5000/5000 [==============================] - 189s 38ms/step - loss: 11.2145 - PSNR: 19.8387\n",
            "5000/5000 [==============================] - 64s 13ms/step\n",
            "5000/5000 [==============================] - 64s 13ms/step\n",
            "[CV]  activation=relu, batch_size=24, learning_rate=0.0005902945472918615, optimizer=<class 'keras.optimizers.Adam'>, score=-11.61787112121582, total=32.6min\n",
            "[CV] activation=relu, batch_size=24, learning_rate=0.0005902945472918615, optimizer=<class 'keras.optimizers.Adam'> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 33.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5000/5000 [==============================] - 192s 38ms/step - loss: 35.5420 - PSNR: 15.1976\n",
            "Epoch 2/10\n",
            "5000/5000 [==============================] - 188s 38ms/step - loss: 20.1921 - PSNR: 17.0797\n",
            "Epoch 3/10\n",
            "5000/5000 [==============================] - 189s 38ms/step - loss: 16.9781 - PSNR: 18.0216\n",
            "Epoch 4/10\n",
            "5000/5000 [==============================] - 189s 38ms/step - loss: 15.0520 - PSNR: 18.6215\n",
            "Epoch 5/10\n",
            "5000/5000 [==============================] - 189s 38ms/step - loss: 13.5888 - PSNR: 19.0103\n",
            "Epoch 6/10\n",
            "5000/5000 [==============================] - 189s 38ms/step - loss: 13.0244 - PSNR: 19.2426\n",
            "Epoch 7/10\n",
            "5000/5000 [==============================] - 189s 38ms/step - loss: 12.3900 - PSNR: 19.4385\n",
            "Epoch 8/10\n",
            "5000/5000 [==============================] - 189s 38ms/step - loss: 11.8815 - PSNR: 19.5958\n",
            "Epoch 9/10\n",
            "5000/5000 [==============================] - 189s 38ms/step - loss: 11.5539 - PSNR: 19.6954\n",
            "Epoch 10/10\n",
            "5000/5000 [==============================] - 189s 38ms/step - loss: 11.5790 - PSNR: 19.7689\n",
            "5000/5000 [==============================] - 64s 13ms/step\n",
            "5000/5000 [==============================] - 64s 13ms/step\n",
            "[CV]  activation=relu, batch_size=24, learning_rate=0.0005902945472918615, optimizer=<class 'keras.optimizers.Adam'>, score=-11.397585940551759, total=32.6min\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] activation=sigmoid, batch_size=8, learning_rate=0.0008989746764255452, optimizer=<class 'keras.optimizers.Adam'> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 67.4min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 67.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5000/5000 [==============================] - 261s 52ms/step - loss: 97.1939 - PSNR: 6.9774\n",
            "Epoch 2/10\n",
            "5000/5000 [==============================] - 257s 51ms/step - loss: 79.9088 - PSNR: 8.3335\n",
            "Epoch 3/10\n",
            "5000/5000 [==============================] - 257s 51ms/step - loss: 66.5119 - PSNR: 9.6621\n",
            "Epoch 4/10\n",
            "5000/5000 [==============================] - 257s 51ms/step - loss: 58.3862 - PSNR: 10.7536\n",
            "Epoch 5/10\n",
            "5000/5000 [==============================] - 257s 51ms/step - loss: 53.8733 - PSNR: 11.4863\n",
            "Epoch 6/10\n",
            "5000/5000 [==============================] - 257s 51ms/step - loss: 51.7354 - PSNR: 11.9562\n",
            "Epoch 7/10\n",
            "5000/5000 [==============================] - 257s 51ms/step - loss: 50.9992 - PSNR: 12.1549\n",
            "Epoch 8/10\n",
            "5000/5000 [==============================] - 257s 51ms/step - loss: 50.7974 - PSNR: 12.2287\n",
            "Epoch 9/10\n",
            "5000/5000 [==============================] - 257s 51ms/step - loss: 50.7512 - PSNR: 12.2653\n",
            "Epoch 10/10\n",
            "5000/5000 [==============================] - 257s 51ms/step - loss: 50.7459 - PSNR: 12.2646\n",
            "5000/5000 [==============================] - 86s 17ms/step\n",
            "5000/5000 [==============================] - 85s 17ms/step\n",
            "[CV]  activation=sigmoid, batch_size=8, learning_rate=0.0008989746764255452, optimizer=<class 'keras.optimizers.Adam'>, score=-50.95882987670898, total=44.3min\n",
            "[CV] activation=sigmoid, batch_size=8, learning_rate=0.0008989746764255452, optimizer=<class 'keras.optimizers.Adam'> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 45.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5000/5000 [==============================] - 261s 52ms/step - loss: 99.2788 - PSNR: 6.8291\n",
            "Epoch 2/10\n",
            "5000/5000 [==============================] - 257s 51ms/step - loss: 84.3793 - PSNR: 7.9455\n",
            "Epoch 3/10\n",
            "5000/5000 [==============================] - 257s 51ms/step - loss: 72.1265 - PSNR: 9.0691\n",
            "Epoch 4/10\n",
            "5000/5000 [==============================] - 257s 51ms/step - loss: 62.6150 - PSNR: 10.1628\n",
            "Epoch 5/10\n",
            "5000/5000 [==============================] - 257s 51ms/step - loss: 56.6613 - PSNR: 11.0391\n",
            "Epoch 6/10\n",
            "5000/5000 [==============================] - 257s 51ms/step - loss: 53.3666 - PSNR: 11.6429\n",
            "Epoch 7/10\n",
            "5000/5000 [==============================] - 256s 51ms/step - loss: 51.8195 - PSNR: 11.9818\n",
            "Epoch 8/10\n",
            "5000/5000 [==============================] - 256s 51ms/step - loss: 51.2050 - PSNR: 12.1425\n",
            "Epoch 9/10\n",
            "5000/5000 [==============================] - 256s 51ms/step - loss: 51.0133 - PSNR: 12.2207\n",
            "Epoch 10/10\n",
            "5000/5000 [==============================] - 256s 51ms/step - loss: 50.9661 - PSNR: 12.2453\n",
            "5000/5000 [==============================] - 85s 17ms/step\n",
            "5000/5000 [==============================] - 84s 17ms/step\n",
            "[CV]  activation=sigmoid, batch_size=8, learning_rate=0.0008989746764255452, optimizer=<class 'keras.optimizers.Adam'>, score=-50.744837567138674, total=44.3min\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] activation=relu, batch_size=24, learning_rate=0.0019950901838622246, optimizer=<class 'keras.optimizers.Adadelta'> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 91.4min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 91.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5000/5000 [==============================] - 199s 40ms/step - loss: 56.8739 - PSNR: 11.7050\n",
            "Epoch 2/10\n",
            "5000/5000 [==============================] - 195s 39ms/step - loss: 34.3202 - PSNR: 14.6837\n",
            "Epoch 3/10\n",
            "5000/5000 [==============================] - 195s 39ms/step - loss: 31.5892 - PSNR: 15.2489\n",
            "Epoch 4/10\n",
            "5000/5000 [==============================] - 195s 39ms/step - loss: 29.3735 - PSNR: 15.6713\n",
            "Epoch 5/10\n",
            "5000/5000 [==============================] - 195s 39ms/step - loss: 27.3395 - PSNR: 16.0002\n",
            "Epoch 6/10\n",
            "5000/5000 [==============================] - 195s 39ms/step - loss: 26.0512 - PSNR: 16.2132\n",
            "Epoch 7/10\n",
            "5000/5000 [==============================] - 195s 39ms/step - loss: 25.1726 - PSNR: 16.3601\n",
            "Epoch 8/10\n",
            "5000/5000 [==============================] - 195s 39ms/step - loss: 24.5530 - PSNR: 16.4615\n",
            "Epoch 9/10\n",
            "5000/5000 [==============================] - 194s 39ms/step - loss: 24.0300 - PSNR: 16.5449\n",
            "Epoch 10/10\n",
            "5000/5000 [==============================] - 193s 39ms/step - loss: 23.6799 - PSNR: 16.6048\n",
            "5000/5000 [==============================] - 65s 13ms/step\n",
            "5000/5000 [==============================] - 64s 13ms/step\n",
            "[CV]  activation=relu, batch_size=24, learning_rate=0.0019950901838622246, optimizer=<class 'keras.optimizers.Adadelta'>, score=-23.65917702331543, total=33.7min\n",
            "[CV] activation=relu, batch_size=24, learning_rate=0.0019950901838622246, optimizer=<class 'keras.optimizers.Adadelta'> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 34.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5000/5000 [==============================] - 197s 39ms/step - loss: 57.8485 - PSNR: 11.7034\n",
            "Epoch 2/10\n",
            "5000/5000 [==============================] - 192s 38ms/step - loss: 33.6241 - PSNR: 14.7601\n",
            "Epoch 3/10\n",
            "5000/5000 [==============================] - 192s 38ms/step - loss: 30.5135 - PSNR: 15.2938\n",
            "Epoch 4/10\n",
            "5000/5000 [==============================] - 192s 38ms/step - loss: 28.6611 - PSNR: 15.6190\n",
            "Epoch 5/10\n",
            "5000/5000 [==============================] - 192s 38ms/step - loss: 27.1123 - PSNR: 15.9355\n",
            "Epoch 6/10\n",
            "5000/5000 [==============================] - 193s 39ms/step - loss: 26.0566 - PSNR: 16.1459\n",
            "Epoch 7/10\n",
            "5000/5000 [==============================] - 193s 39ms/step - loss: 25.2901 - PSNR: 16.2891\n",
            "Epoch 8/10\n",
            "5000/5000 [==============================] - 193s 39ms/step - loss: 24.8017 - PSNR: 16.3695\n",
            "Epoch 9/10\n",
            "5000/5000 [==============================] - 193s 39ms/step - loss: 24.4076 - PSNR: 16.4430\n",
            "Epoch 10/10\n",
            "5000/5000 [==============================] - 193s 39ms/step - loss: 24.1187 - PSNR: 16.4919\n",
            "5000/5000 [==============================] - 65s 13ms/step\n",
            "5000/5000 [==============================] - 64s 13ms/step\n",
            "[CV]  activation=relu, batch_size=24, learning_rate=0.0019950901838622246, optimizer=<class 'keras.optimizers.Adadelta'>, score=-23.82859783630371, total=33.3min\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] activation=sigmoid, batch_size=8, learning_rate=0.005341382720091342, optimizer=<class 'keras.optimizers.Adagrad'> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 69.1min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 69.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5000/5000 [==============================] - 250s 50ms/step - loss: 98.6608 - PSNR: 6.8786\n",
            "Epoch 2/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 92.8822 - PSNR: 7.2847\n",
            "Epoch 3/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 89.2833 - PSNR: 7.5493\n",
            "Epoch 4/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 86.4397 - PSNR: 7.7739\n",
            "Epoch 5/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 84.1039 - PSNR: 7.9639\n",
            "Epoch 6/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 82.0901 - PSNR: 8.1337\n",
            "Epoch 7/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 80.3139 - PSNR: 8.2828\n",
            "Epoch 8/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 78.7236 - PSNR: 8.4159\n",
            "Epoch 9/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 77.2830 - PSNR: 8.5476\n",
            "Epoch 10/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 75.9666 - PSNR: 8.6789\n",
            "5000/5000 [==============================] - 85s 17ms/step\n",
            "5000/5000 [==============================] - 83s 17ms/step\n",
            "[CV]  activation=sigmoid, batch_size=8, learning_rate=0.005341382720091342, optimizer=<class 'keras.optimizers.Adagrad'>, score=-75.62238933105469, total=42.4min\n",
            "[CV] activation=sigmoid, batch_size=8, learning_rate=0.005341382720091342, optimizer=<class 'keras.optimizers.Adagrad'> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 43.8min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5000/5000 [==============================] - 250s 50ms/step - loss: 100.7495 - PSNR: 6.7197\n",
            "Epoch 2/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 95.6938 - PSNR: 7.0685\n",
            "Epoch 3/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 92.6098 - PSNR: 7.2970\n",
            "Epoch 4/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 90.0468 - PSNR: 7.4824\n",
            "Epoch 5/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 87.9256 - PSNR: 7.6503\n",
            "Epoch 6/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 86.0989 - PSNR: 7.7963\n",
            "Epoch 7/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 84.4771 - PSNR: 7.9298\n",
            "Epoch 8/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 83.0125 - PSNR: 8.0524\n",
            "Epoch 9/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 81.6749 - PSNR: 8.1532\n",
            "Epoch 10/10\n",
            "5000/5000 [==============================] - 245s 49ms/step - loss: 80.4427 - PSNR: 8.2719\n",
            "5000/5000 [==============================] - 85s 17ms/step\n",
            " 272/5000 [>.............................] - ETA: 1:18Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "09UH2-4DJYwZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model = KerasRegressor(build_fn=get_unet_model, epochs=20, batch_size=8)\n",
        "# model.fit(x=x, y=y)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}